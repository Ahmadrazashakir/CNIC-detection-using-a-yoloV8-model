{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2bDecE5XVaq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deIXeCKT0lMu"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1kd-GLDDDme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5006ac2-ff8b-4758-f99a-094511a3567f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDy2PliNopxD"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjoKR2kgovHz"
      },
      "source": [
        "### Download and unzip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yGRSY67Brcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea142f3-83f1-4335-f1a9-a4e1630912e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1BqkjrY4VzghDFKlMGE-gafrqCvmVlm3f \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1BqkjrY4VzghDFKlMGE-gafrqCvmVlm3f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOfRXdwCii8",
        "outputId": "4c7c8987-04b7-4f83-c1e6-b6c25715f566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/scenery.zip, /content/scenery.zip.zip or /content/scenery.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/scenery.zip;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kHN1fsIVosW"
      },
      "source": [
        "### Loading and dividing the dataset into train, val and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al94cia4o_gz"
      },
      "source": [
        "Initializing ImageFolder Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTMLveJzo3q_",
        "outputId": "bc44f4fd-0a8a-4558-e49e-f319d2a6ca53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9827940c91d7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/scenery/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#transform/load data, indexes and loads up the set of sub-dirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/scenery/'"
          ]
        }
      ],
      "source": [
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Resize((90,90))]) #resize images and load into tensor\n",
        "\n",
        "data_path = '/content/scenery/'\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root=data_path, transform=transform) #transform/load data, indexes and loads up the set of sub-dirs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBx2rLb52wbn"
      },
      "source": [
        "Attributes of the dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvbJp7j6aq1C"
      },
      "outputs": [],
      "source": [
        "len(dataset) # number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM47K9a5pQYE"
      },
      "outputs": [],
      "source": [
        "dataset.classes # classes(build in?????, figure this out later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqAUDoIrpWfz"
      },
      "outputs": [],
      "source": [
        "dataset.class_to_idx # indices of corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqIbsUMqQAD"
      },
      "outputs": [],
      "source": [
        "dataset.transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD4QBKgWYVZa"
      },
      "outputs": [],
      "source": [
        "class_counts = np.zeros(len(dataset.class_to_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cWNITZ7YTpT"
      },
      "outputs": [],
      "source": [
        "for image, label in dataset:\n",
        "    class_counts[label] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5saTJB9YNBc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(class_counts)):\n",
        "  print(\"class:%s, instances: %d\"%([k for k,v in dataset.class_to_idx.items() if v == i], class_counts[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcfgdTe8_zLv"
      },
      "outputs": [],
      "source": [
        "def display_img(img,label):\n",
        "    #print(f\"Label : {dataset.classes[label]}\")\n",
        "    plt.figure()\n",
        "    plt.title(f\"Label : {dataset.classes[label]}\")\n",
        "    plt.imshow(img.permute(1,2,0))\n",
        "\n",
        "#display the first image in the dataset\n",
        "for i in range(1,4):\n",
        "  display_img(*dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuyqw6Zkp7ta"
      },
      "source": [
        "Split into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgrSoF1-p6I7"
      },
      "outputs": [],
      "source": [
        "# test train split, utils package, hard coded??? takes fractions??? check later,,,,, manual seed >> random seeding!!!\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [11924, 2555, 2555], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaLxXD0CbO6T"
      },
      "outputs": [],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE-xKCe7sMTz"
      },
      "outputs": [],
      "source": [
        "train_set.indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "136L2ya1qsU6"
      },
      "outputs": [],
      "source": [
        "train_set.dataset # dataset means train set is a part/subset of the dataset!,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOo9uSc1_2sb"
      },
      "outputs": [],
      "source": [
        "def display_img(img,label):\n",
        "    print(f\"Label : {dataset.classes[label]}\")\n",
        "    plt.imshow(img.permute(1,2,0))\n",
        "\n",
        "#display the first image in the dataset\n",
        "display_img(*train_set[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGVbqC8dpFOz"
      },
      "source": [
        "Initializing the pytorch dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOV5lugliOlZ"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj2Vq6ctB5aB"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_batch(loader):\n",
        "    \"\"\"Plot images grid of single batch\"\"\"\n",
        "    for images, labels in loader:\n",
        "        fig,ax = plt.subplots(figsize = (16,12))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
        "        break\n",
        "\n",
        "show_batch(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LARAhY3QVjJr"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2x7RF37za1v"
      },
      "source": [
        "## nn.Module method of constructing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4uGfuLmtWLF"
      },
      "outputs": [],
      "source": [
        "#TF>> sequential - simple\n",
        "#  >> modular!! >> complex(flexible)\n",
        "\n",
        "\n",
        "\n",
        "#Modular way!!!!\n",
        "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__() #init parent class\n",
        "\n",
        "        # Defining a 2D convolution layer\n",
        "        self.conv1 = Conv2d(3, 4, kernel_size=3, stride=1, padding=1)#(input channels, output channels,------)\n",
        "        self.bn1 = BatchNorm2d(4)\n",
        "        self.relu1 = ReLU(inplace=True) ## note thr inplace, is it important????\n",
        "        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Defining another 2D convolution layer\n",
        "        self.conv2 = Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = BatchNorm2d(8)\n",
        "        self.relu2 = ReLU(inplace=True)\n",
        "        self.maxpool2 = MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = BatchNorm2d(16)\n",
        "        self.relu3 = ReLU(inplace=True)\n",
        "        self.maxpool3 = MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = BatchNorm2d(32)\n",
        "        self.relu4 = ReLU(inplace=True)\n",
        "        self.maxpool4 = MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        #linear(input,output)\n",
        "        self.linear_layers = Linear(32 * 5 * 5, 6)  ## flatten the image here. i.e linear layer!!\n",
        "\n",
        "    # Defining the forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # Apply conv2, bn2, relu2 and maxpool2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # Apply conv3, bn3, relu3 and maxpool3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        # Apply conv4, bn4, relu4 and maxpool4\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        # Flatten the output from the conv layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply the linear layer\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9HrwTzcBIAU"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = Net()\n",
        "model.to(device)  # to GPU/CPU!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIFTgITxzhdP"
      },
      "source": [
        "## sequential method of constructing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGwKfFe_zCgR"
      },
      "outputs": [],
      "source": [
        "# sequential way\n",
        "\n",
        "model = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(3, 4, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(4),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv2d(4, 8, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(8),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(32),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            Linear(32 * 5 * 5, 6)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAjhyCjGg-AQ"
      },
      "source": [
        "# Visualization of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58z-pCo2oFMk"
      },
      "source": [
        "### Visualize through torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxIZg8dqg8Pq"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPqt1iSzmCZ1"
      },
      "outputs": [],
      "source": [
        "dummy_model = Net().to(device)\n",
        "summary(dummy_model, (3, 90, 90))  #-1 correspnds to batch size!, -1>> last or all remaining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-96Fi2oBDL"
      },
      "source": [
        "### Visualize through TorchViz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxE3a9HSn_QS"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp-TVTR-hIyZ"
      },
      "outputs": [],
      "source": [
        "from torchviz import make_dot\n",
        "dummy_image = next(iter(train_loader))[0]\n",
        "dummy_model = Net()\n",
        "y_hat = dummy_model(dummy_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glcn9En7i3Dg"
      },
      "outputs": [],
      "source": [
        "make_dot(y_hat.mean(),params=dict(dummy_model.named_parameters())).render(\"graph2\", format=\"png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNfWa0ALyUhO"
      },
      "outputs": [],
      "source": [
        "dummy_model.state_dict() # show the entire as dict!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lg93xOdzvp9"
      },
      "source": [
        "### Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQAd4sp9z3XV"
      },
      "outputs": [],
      "source": [
        "# Select a loss function\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Select an optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmrFwrmbVzfC"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIwea2XD2F31"
      },
      "source": [
        "Variable Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq4huRgFgFxL"
      },
      "outputs": [],
      "source": [
        "len(train_loader) # number of images per batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_aHSRvmyplY"
      },
      "outputs": [],
      "source": [
        "len(train_loader.dataset.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQiBlut4cTpU"
      },
      "outputs": [],
      "source": [
        "def run_1_epoch(model, loss_fn, optimizer, loader, train = False):\n",
        "\n",
        "\n",
        "  if train:\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()\n",
        "\n",
        "  total_correct_preds = 0\n",
        "\n",
        "  total_samples_in_loader = len(loader.dataset.indices)\n",
        "  total_batches_in_loader = len(loader)\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for image_batch, labels in tqdm(loader): #TPDM gives you proegress updates!\n",
        "\n",
        "    # Transfer image_batch to GPU if available\n",
        "    image_batch = image_batch.to(device) #gives you images of the batch\n",
        "    labels = labels.to(device) #gives you labels of the batch\n",
        "\n",
        "    # Zeroing out the gradients for parameters\n",
        "    if train:\n",
        "      optimizer.zero_grad() # pytorch doesnt reset gradients to zeros after each iteration and keeps adding to the previous,\n",
        "      #if this is not done\n",
        "\n",
        "    # Forward pass on the input batch\n",
        "    output = model.forward(image_batch)\n",
        "\n",
        "    # Acquire predicted class indices\n",
        "    _, predicted = torch.max(output.data, 1) # the dimension 1 corresponds to max along the rows\n",
        "\n",
        "\n",
        "    # Removing extra last dimension from output tensor\n",
        "    output.squeeze_(-1)\n",
        "\n",
        "    # Compute the loss for the minibatch\n",
        "    loss = loss_function(output, labels)\n",
        "\n",
        "    # Backpropagation\n",
        "    if train:\n",
        "      loss.backward()\n",
        "\n",
        "    # Update the parameters using the gradients\n",
        "    if train:\n",
        "      optimizer.step()\n",
        "\n",
        "    # Extra variables for calculating loss and accuracy\n",
        "    # count total predictions for accuracy calcutuon for this epoch\n",
        "    total_correct_preds += (predicted == labels).sum().item()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  loss = total_loss / total_batches_in_loader\n",
        "  accuracy = 100 * total_correct_preds / total_samples_in_loader\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipQEneNVcNe7"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "\n",
        "#train_accuracy_list = []\n",
        "#val_accuracy_list = []\n",
        "\n",
        "#train_loss_list = []\n",
        "#val_loss_list = []\n",
        "\n",
        "#val_accuracy_max = -1 # used to store best model based on accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1B_t0X9gPmX"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ABSok0mcajB"
      },
      "outputs": [],
      "source": [
        "# Main training and validation loop for n number of epochs\n",
        "for i in range(epochs):\n",
        "\n",
        "  # Train model for one epoch\n",
        "  print(\"Epoch %d: Train\"%(i))\n",
        "  train_loss, train_accuracy  = run_1_epoch(model, loss_function, optimizer, train_loader, train= True)\n",
        "\n",
        "  # Lists for train loss and accuracy for plotting\n",
        "  train_loss_list.append(train_loss)\n",
        "  train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "  # Validate the model on validation set\n",
        "  print(\"Epoch %d: Validation\"%(i))\n",
        "  with torch.no_grad():\n",
        "    val_loss, val_accuracy  = run_1_epoch(model, loss_function, optimizer, val_loader, train= False)\n",
        "\n",
        "  # Lists for val loss and accuracy for plotting\n",
        "  val_loss_list.append(val_loss)\n",
        "  val_accuracy_list.append(val_accuracy)\n",
        "\n",
        "  print('train loss: %.4f'%(train_loss))\n",
        "  print('val loss: %.4f'%(val_loss))\n",
        "  print('train_accuracy %.2f' % (train_accuracy))\n",
        "  print('val_accuracy %.2f' % (val_accuracy))\n",
        "\n",
        "  # Save model if validation accuracy for current epoch is greater than\n",
        "  # all the previous epochs\n",
        "  if val_accuracy > val_accuracy_max:\n",
        "    val_accuracy_max = val_accuracy\n",
        "    print(\"New Max val Accuracy Acheived %.2f. Saving model.\\n\\n\"%(val_accuracy_max))\n",
        "    torch.save(model,'best_val_acc_model.pth')\n",
        "  else:\n",
        "    print(\"val accuracy did not increase from %.2f\\n\\n\"%(val_accuracy_max))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHqKygtNV_hm"
      },
      "source": [
        "**Accuracy and Loss Result Graphs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbCQgAjmDKtA"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(train_accuracy_list, label=\"train_accuracy\")\n",
        "plt.plot(val_accuracy_list, label=\"val_accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.title('Training and val Accuracy')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss_list, label=\"train_loss\")\n",
        "plt.plot(val_loss_list, label=\"val_loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and val Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBdWG_iQvLM4"
      },
      "source": [
        "# Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG8O7jt9WJbn"
      },
      "source": [
        "**Loading the best saved model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN-I9xZ9urOc"
      },
      "outputs": [],
      "source": [
        "best_val_model1 = torch.load('/content/best_val_acc_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHOX3pjBK4MV"
      },
      "outputs": [],
      "source": [
        "torch.save(best_val_model1,'best_val_acc_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kobqLECOvKuI"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    test_loss, test_accuracy  = run_1_epoch(model, loss_function, optimizer, test_loader, train= False)\n",
        "\n",
        "print('test loss: %.4f'%(train_loss))\n",
        "print('test_accuracy %.2f' % (train_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvv2nWMCWeoH"
      },
      "source": [
        "**Scene Sample Inferences:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7kiwlKFki6C"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/Machvis/Labs/Lab1_pytorch_intro/samples_scenery.zip ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0y8bRIgxDEs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Copying the Pokemon Sample Folder from the drive into the colab disk\n",
        "!unzip /content/samples_scenery.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWFwYFAupnTw"
      },
      "outputs": [],
      "source": [
        "#Loop for loading each image and predict the out for each one\n",
        "#The title of each image is the inference result of the model\n",
        "pokemon_samples_path = '/content/samples_scenery'\n",
        "for image_path in os.listdir(pokemon_samples_path):\n",
        "  try:\n",
        "    im = Image.open(os.path.join(pokemon_samples_path, image_path))\n",
        "    im = im.convert(\"RGB\")\n",
        "    im = im.resize((90,90),Image.ANTIALIAS)\n",
        "    loader = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "    im_tensor = loader(im)\n",
        "    im_tensor = im_tensor.view(1,3,90,90)\n",
        "    im_tensor = im_tensor.to(device)  #assumes that you're using GPU\n",
        "    output = best_val_model1.forward(im_tensor)\n",
        "    #Get the class-to-index dictionary\n",
        "    class_to_index_dict = train_loader.sampler.data_source.dataset.class_to_idx\n",
        "    #Get the predicted class label\n",
        "    prediction_label = output.argmax()\n",
        "    plt.figure()\n",
        "    #Print the title corresponding to the predicted label\n",
        "    plt.title([k for k,v in class_to_index_dict.items() if v == prediction_label],fontweight=\"bold\")\n",
        "    plt.imshow(im)\n",
        "  except IOError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVQ86xAQLKdL"
      },
      "source": [
        "# Credits\n",
        "Notebook Prepared by Bostan Khan, Team Lead, MachVIS Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvwxXALhHGSM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}